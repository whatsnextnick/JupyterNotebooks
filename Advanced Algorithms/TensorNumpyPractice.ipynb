{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc63dc1d-1a43-411d-915a-404bff84dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\njmon\\OneDrive\\Desktop\\Python_Data_Science_Notes\\JupyterNotebooks\\myenv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d30f309-aec2-4356-8bfc-f7149119bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\njmon\\OneDrive\\Desktop\\Python_Data_Science_Notes\\JupyterNotebooks\\myenv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[200, 17]]) #2d array \n",
    "layer_1 = Dense(units = 3, activation = 'sigmoid') #creating a layer with 3 neurons which is activated by sigmoid\n",
    "a_1 = layer_1(x) #output of a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca48c85-d691-4da8-ae25-26367e5274e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0. 1. 0.]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfddc8e8-cf00-4cfe-97ed-6fe4132b1aeb",
   "metadata": {},
   "source": [
    "Forward Propagation: Hard Coding w/ Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86bc99bd-37df-42bf-a5f6-7946546ba267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([200, 17]) #input array\n",
    "\n",
    "def sigmoid(z): \n",
    "    sig = 1 /( 1 + np.exp(-z))\n",
    "    return sig\n",
    "    \n",
    "\n",
    "w1_1 = np.array([1, 3]) #Weight for first neuron in the first layer\n",
    "b1_1 = np.array([1]) #Corresponding bias\n",
    "z1_1 = np.dot(w1_1,x) + b1_1 #z = f(x)= w dot x +b \n",
    "a1_1 = sigmoid(z1_1) #activation via sigmoid \n",
    "print(a1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2620cfd-442d-4717-9623-ffd499b13f55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m         a_out[j] \u001b[38;5;241m=\u001b[39m sigmoid(z)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a_out\n\u001b[1;32m---> 18\u001b[0m \u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 16\u001b[0m, in \u001b[0;36mdense\u001b[1;34m(W, a_in, b)\u001b[0m\n\u001b[0;32m     14\u001b[0m     w \u001b[38;5;241m=\u001b[39m W[:,j] \u001b[38;5;66;03m# a way to get columns or feature values from W\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(w,a_in) \u001b[38;5;241m+\u001b[39m b \n\u001b[1;32m---> 16\u001b[0m     \u001b[43ma_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m sigmoid(z)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a_out\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "W = np.array([ #These are the output values of the previous layer neurons 's 1,2 ,  3,4,   5,6\n",
    "    [1, 3, 5],\n",
    "    [2, 4, 6]])\n",
    "\n",
    "b = np.array([1, 2, 3])\n",
    "a_1 = np.array([4, 5]) #a1 ouput values #a1 output values\n",
    "\n",
    "#define a function to take in a1 into next layer, a2\n",
    "def dense(W, a_in, b):\n",
    "    units = W.shape[1] # getting number of neurons\n",
    "    a_out = np.zeros(units) #initializing the a_out variable -> [0,0,0]\n",
    "    #for loop to iterate over a_out and input a2_j values\n",
    "    for j in range(units):\n",
    "        w = W[:,j] # a way to get columns or feature values from W\n",
    "        z = np.dot(w,a_in) + b \n",
    "        a_out[j] = sigmoid(z)\n",
    "    return a_out\n",
    "dense(W, a_1, b)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7399886-e046-4657-a92f-851c1e112d67",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
